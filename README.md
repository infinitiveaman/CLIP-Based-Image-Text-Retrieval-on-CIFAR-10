CLIP-Based Image-Text Retrieval on CIFAR-10 | PyTorch, OpenAI CLIP, Google Colab
• Implemented a multimodal image-text search engine using the CLIP model on the CIFAR-10 dataset.
• Enabled two-way retrieval: finding similar CIFAR-10 images from a user-uploaded image or a natural language text query.
• Leveraged CLIP's image and text embeddings to compute cosine similarity for nearest neighbor retrieval.
• Built an interactive interface on Google Colab to visualize top-k similar images and query results.
• Preprocessed and encoded all dataset images in advance for efficient search using matrix operations in PyTorch.
